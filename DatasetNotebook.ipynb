{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. DATASET DOWNLOAD**"
      ],
      "metadata": {
        "id": "TjsH4u2aeJlr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnFjnuvxeIBA",
        "outputId": "11ae9111-9e27-49fe-ac91-684a970c36a0",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Dataset ready to use\n"
          ]
        }
      ],
      "source": [
        "# @title **1.1 Dataset download and extraction**\n",
        "\n",
        "import os\n",
        "import gdown\n",
        "\n",
        "%cd /content\n",
        "\n",
        "dataset_dir = \"/content/datasets/linemod\"  # absolute path\n",
        "\n",
        "# Check if dataset is already present\n",
        "if os.path.exists(dataset_dir):\n",
        "    print(\"Dataset ready to use\")\n",
        "else:\n",
        "    # Create dataset directory if not exists\n",
        "    !mkdir -p datasets/linemod/\n",
        "    %cd datasets/linemod/\n",
        "\n",
        "    # Download dataset zip from Google Drive\n",
        "    !gdown --fuzzy https://drive.google.com/file/d/1qQ8ZjUI6QauzFsiF8EpaaI2nKFWna_kQ/view?usp=drive_link -O Linemod_preprocessed.zip\n",
        "    !unzip Linemod_preprocessed.zip\n",
        "\n",
        "    print(\"Dataset downloaded and extracted\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. CUSTOM DATASET**"
      ],
      "metadata": {
        "id": "pCoK51xcN3Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **2.1 Custom dataset #1 code - Single object loader**\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import numpy as np\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "class LineModDetectionDataset(Dataset):\n",
        "  def __init__(self, img_dir, gt_path, transform=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        img_dir (string): Path to the folder containing images (e.g. '.../data/01/rgb')\n",
        "        gt_path (string): Full path to the gt.yml file (e.g. '.../data/01/gt.yml')\n",
        "        transform (callable, optional): Optional transform to be applied on a sample.\n",
        "    \"\"\"\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "\n",
        "    # 1. Load the Ground Truth\n",
        "    if not os.path.exists(gt_path):\n",
        "      raise FileNotFoundError(f\"gt.yml not found at {gt_path}\")\n",
        "\n",
        "    with open(gt_path, 'r') as f:\n",
        "      # Load dictionary and force keys to be integers\n",
        "      self.gt_data = {int(k): v for k, v in yaml.safe_load(f).items()}\n",
        "\n",
        "    # 2. Create a list of valid Image IDs\n",
        "    self.image_ids = sorted(list(self.gt_data.keys()))\n",
        "\n",
        "  def __len__(self):\n",
        "    # Returns the total number of samples\n",
        "    return len(self.image_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # This method retrieves the 'idx'-th sample from the dataset\n",
        "\n",
        "    # A. Determine Image ID\n",
        "    img_id = self.image_ids[idx]\n",
        "\n",
        "    # B. Load Image using self.img_dir\n",
        "    # Filenames are like '0000.png', '0001.png'.\n",
        "    # We format the ID with ':04d', which means: pad the imgID with zeros until it is 4 digits long\n",
        "    img_name = f\"{img_id:04d}.png\"\n",
        "\n",
        "    # Join the specific image folder with the filename\n",
        "    img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "    # Read with OpenCV\n",
        "    image = cv2.imread(img_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "\n",
        "    # Convert BGR to RGB\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # B. Get Bounding Box Label as NumPy Array\n",
        "    # bbox -> [x_top_left, y_top_left, width, height]\n",
        "    obj_data = self.gt_data[img_id][0]\n",
        "    bbox = np.array(obj_data['obj_bb'], dtype=np.float32)\n",
        "\n",
        "    # C. Apply Transforms\n",
        "    if self.transform:\n",
        "      # The 'ToTensor' conversion will be applied inside the transform chain\n",
        "      image = self.transform(image)\n",
        "    else:\n",
        "      # 'ToTensor' conversion\n",
        "      image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "    # D. Return the Python Dictionary of the sample\n",
        "    return {\n",
        "        'image_id': img_id,             # Integer\n",
        "        'image': image,                 # PyTorch Tensor [3, Height, Width] -> [3, 480, 640]\n",
        "        'bbox': bbox,                   # NumPy Array                       -> [x_top_left, y_top_left, width, height]\n",
        "        'obj_id': obj_data['obj_id']    # Integer\n",
        "    }"
      ],
      "metadata": {
        "id": "-1G0yOw6N6BG",
        "cellView": "form"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **2.2 Dataset instance**\n",
        "\n",
        "# Define the separate paths\n",
        "path_to_images = \"/content/datasets/linemod/Linemod_preprocessed/data/01/rgb\"\n",
        "path_to_gt = \"/content/datasets/linemod/Linemod_preprocessed/data/01/gt.yml\"\n",
        "\n",
        "# Create the dataset\n",
        "train_dataset = LineModDetectionDataset(\n",
        "    img_dir=path_to_images,\n",
        "    gt_path=path_to_gt\n",
        ")\n",
        "\n",
        "# Create the loader\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True)\n",
        "\n",
        "print(f\"Successfully loaded {len(train_dataset)} images.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "xnnRszBCVprU",
        "outputId": "f7fb1ef0-a344-4bd1-fe7f-a309af03dac6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 1236 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **2.3 Dataset test**\n",
        "\n",
        "# Get one sample to check\n",
        "sample = train_dataset[0]\n",
        "print(f\"ImageID:\\t {sample['image_id']}\")\n",
        "print(f\"ObjectID:\\t {sample['obj_id']}\")\n",
        "print(f\"Image Shape:\\t {sample['image'].shape}\")\n",
        "print(f\"Bounding Box:\\t {sample['bbox']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "F0Xv4klSVxuy",
        "outputId": "45b59817-74b9-4a29-a0a2-e99be26019b3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ImageID:\t 0\n",
            "ObjectID:\t 1\n",
            "Image Shape:\t torch.Size([3, 480, 640])\n",
            "Bounding Box:\t [244. 150.  44.  58.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **2.4 Custom dataset #2 code - Multi objects loader**\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "class MultiObjectLineModDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Path to the main 'data' folder containing subfolders '01', '02', etc.\n",
        "            transform (callable, optional): Optional transform.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # This list will store metadata for EVERY image found in ALL folders\n",
        "        # Format: [ {'path': '/.../01/rgb/0000.png', 'bbox': [...], 'obj_id': 1}, ... ]\n",
        "        self.all_samples = []\n",
        "\n",
        "        print(\"Scanning dataset folders...\")\n",
        "\n",
        "        # 1. Loop through possible object folders (1 to 15)\n",
        "        # LineMOD typically has 15 objects. We iterate to find them.\n",
        "        for i in range(1, 16):\n",
        "            folder_name = f\"{i:02d}\" # Converts 1 -> \"01\", 2 -> \"02\"\n",
        "            folder_path = os.path.join(root_dir, folder_name)\n",
        "\n",
        "            # Check if folder exists (This automatically skips missing Object 3)\n",
        "            if not os.path.exists(folder_path):\n",
        "                continue\n",
        "\n",
        "            print(f\"Loading Object {folder_name}...\")\n",
        "\n",
        "            # 2. Load the GT file for THIS specific folder\n",
        "            gt_path = os.path.join(folder_path, \"gt.yml\")\n",
        "            if not os.path.exists(gt_path):\n",
        "                print(f\"Warning: gt.yml missing in {folder_name}\")\n",
        "                continue\n",
        "\n",
        "            with open(gt_path, 'r') as f:\n",
        "                # Load and force integer keys\n",
        "                folder_gt = yaml.safe_load(f)\n",
        "\n",
        "            # 3. Process all images in this folder\n",
        "            # We iterate through the keys in the GT file (which correspond to image IDs)\n",
        "            for img_id, objects in folder_gt.items():\n",
        "                # LineMOD images usually have 1 object per image, but gt is a list.\n",
        "                obj_data = objects[0]\n",
        "\n",
        "                # Construct full image path\n",
        "                img_filename = f\"{img_id:04d}.png\"\n",
        "                img_full_path = os.path.join(folder_path, \"rgb\", img_filename)\n",
        "\n",
        "                # Extract Data\n",
        "                bbox = np.array(obj_data['obj_bb'], dtype=np.float32)\n",
        "                obj_id = int(obj_data['obj_id'])\n",
        "\n",
        "                # Store everything needed to load this sample later\n",
        "                self.all_samples.append({\n",
        "                    'path': img_full_path,\n",
        "                    'bbox': bbox,\n",
        "                    'obj_id': obj_id,\n",
        "                    'original_img_id': img_id # Useful for debugging\n",
        "                })\n",
        "\n",
        "        #print(f\"Total images loaded: {len(self.all_samples)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # 1. Retrieve the metadata we stored in __init__\n",
        "        sample_info = self.all_samples[idx]\n",
        "\n",
        "        # 2. Load Image from the stored full path\n",
        "        image = cv2.imread(sample_info['path'])\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"Image read error: {sample_info['path']}\")\n",
        "\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # 3. Apply Transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        # 4. Return\n",
        "        return {\n",
        "            'image': image,                             # The torch.FloatTensor ->  [3, 480, 640]\n",
        "            'bbox': sample_info['bbox'],                # NumPy Array           ->  [x_top_left, y_top_left, width, height]\n",
        "            'obj_id': sample_info['obj_id'],            # The class (e.g., 1 for Ape)\n",
        "            'image_id': sample_info['original_img_id'], # The image id for the class (e.g., 0)\n",
        "            'image_path': sample_info['path']           # The file location\n",
        "        }\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5rdL6ssb0XcA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **2.5 Multiobject dataset instance**\n",
        "\n",
        "dataset_root = \"/content/datasets/linemod/Linemod_preprocessed/data\"\n",
        "\n",
        "# 1. Define the Transform Pipeline\n",
        "# ToTensor(): Converts Numpy (H,W,C) 0-255 -> Tensor (C,H,W) 0.0-1.0\n",
        "# Normalize(): Subtracts Mean and divides by Std (Standard ImageNet values)\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 2. Create Dataset\n",
        "full_train_dataset = MultiObjectLineModDataset(\n",
        "    root_dir=dataset_root,\n",
        "    transform=data_transform\n",
        ")\n",
        "\n",
        "# 3. Create Loader\n",
        "train_loader = DataLoader(full_train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# 4. Output\n",
        "print(f\"Successfully loaded {len(full_train_dataset)} images.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "HViUgvSp5Rvi",
        "outputId": "276c9199-ceda-4cef-ee93-9676fe8449ec"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning dataset folders...\n",
            "Loading Object 01...\n",
            "Loading Object 02...\n",
            "Loading Object 04...\n",
            "Loading Object 05...\n",
            "Loading Object 06...\n",
            "Loading Object 08...\n",
            "Loading Object 09...\n",
            "Loading Object 10...\n",
            "Loading Object 11...\n",
            "Loading Object 12...\n",
            "Loading Object 13...\n",
            "Loading Object 14...\n",
            "Loading Object 15...\n",
            "Total images loaded: 15800\n",
            "Successfully loaded 15800 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **2.6 Multiobject dataset test**\n",
        "\n",
        "random_sample_id = torch.randint(low=0, high=len(full_train_dataset), size=(1,))\n",
        "\n",
        "# Get one sample to check\n",
        "sample = full_train_dataset[random_sample_id]\n",
        "print(f\"ImageID:\\t {sample['image_id']}\")\n",
        "print(f\"ObjectID:\\t {sample['obj_id']}\")\n",
        "print(f\"Image Shape:\\t {sample['image'].shape}\")\n",
        "print(f\"Bounding Box:\\t {sample['bbox']}\")\n",
        "print(f\"Image path:\\t {sample['image_path']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "H_ahQtAz6xXZ",
        "outputId": "d5ae3510-3f1a-44b1-f83b-3d8a69217523"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ImageID:\t 227\n",
            "ObjectID:\t 15\n",
            "Image Shape:\t torch.Size([3, 480, 640])\n",
            "Bounding Box:\t [298. 156.  84. 112.]\n",
            "Image path:\t /content/datasets/linemod/Linemod_preprocessed/data/15/rgb/0227.png\n"
          ]
        }
      ]
    }
  ]
}